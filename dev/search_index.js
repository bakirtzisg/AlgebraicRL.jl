var documenterSearchIndex = {"docs":
[{"location":"#AlgebraicRL-Documentation","page":"AlgebraicRL Documentation","title":"AlgebraicRL Documentation","text":"","category":"section"},{"location":"","page":"AlgebraicRL Documentation","title":"AlgebraicRL Documentation","text":"MDPAgent\nsolve(mdpAgent::MDPAgent, display = false)\nBase.collect(mdpagent::MDPAgent) \ndefault_output_function(env::AbstractEnv)\ndefault_input_function(env::AbstractEnv, internal_state::Vector)\nMDPAgentMachine(    env::AbstractEnv, agent::AbstractPolicy,\n                        ;input_size = 1, input_function = default_input_function,\n                        output_size = 1, output_function = default_output_function,\n                        display = false, resetMDP = true\n                        )\nAlgebraicDynamics.DWDDynam.eval_dynamics(f::AbstractMachine, xs=[0.0])\nread_output(f::AbstractMachine)\n\n","category":"page"},{"location":"#AlgebraicRL.MDPAgent","page":"AlgebraicRL Documentation","title":"AlgebraicRL.MDPAgent","text":"mutable struct MDPAgent\n    mdp::AbstractEnv\n    agent::AbstractPolicy\nend\n\nDefines a discrete machine that consists of an MDP and the agent capable of solving it. Running solve() will iterate through 1 episode of the MDP and return a done signal. The MDP follows the specification from ReinforcementLearning.jl. Each MDP, of type AbstractEnv, has the following functions:\n\naction_space(env::YourEnv)\nstate(env::YourEnv)\nstate_space(env::YourEnv)\nreward(env::YourEnv)\nis_terminated(env::YourEnv)\nreset!(env::YourEnv)\n(env::YourEnv)(action) # note this function applies your action to the environment and steps forward in time.\n\nSee https://juliareinforcementlearning.org/docs/Howtowriteacustomized_environment/ for more details on how to write a custom environment. Alternatively, see https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/tree/master/src/ReinforcementLearningExperiments/deps/experiments/experiments for examples of used environments. \n\nThe agent also comes from ReinforcementLearning.jl. See https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/61256bcf1c493914d5003f22e126c997332c2c39/src/ReinforcementLearningBase/src/interface.jl for a full definition. Mainly, it defines the following functions:\n\n(policy::AbstractPolicy)(env) = returns an action from the policy given the environment\nupdate(policy::AbstractPolicy, experience) = updates the policy based on an online or offline experience\n\n\n\n\n\n","category":"type"},{"location":"#AlgebraicRL.solve","page":"AlgebraicRL Documentation","title":"AlgebraicRL.solve","text":"solve(mdpAgent::MDPAgent, display = false)\n\nRuns 1 episode of the environment under the policy provided by the agent.\n\nArguments:\n\nmdpAgent - The MDPAgent you constructed, which contains an MDP following the AbstractEnv interface and an agent following the AbstractPolicy interface.\ndisplay - Whether or not to display the environment each step. Displaying is done via calling Base.display, so overwrite Base.display(yourEnv) in order to view your environment being solved.\n\nReturns:\n\nThe accumulated reward over the episode as returned by the environment\n\n\n\n\n\n","category":"function"},{"location":"#Base.collect-Tuple{MDPAgent}","page":"AlgebraicRL Documentation","title":"Base.collect","text":"Base.collect(mdpagent::MDPAgent)\n\nOverwrites base.collect for our class since it will be used by Algebraic dynamics. Simply returns the mdpAgent since there is nothing to collect. Note collect is usually used to return an array of elements from a collection or iterator \n\n\n\n\n\n","category":"method"},{"location":"#AlgebraicRL.default_output_function-Tuple{AbstractEnv}","page":"AlgebraicRL Documentation","title":"AlgebraicRL.default_output_function","text":"default_output_function(env::AbstractEnv)\n\nReturns the result of the mdp episode. Is it done or not?  To do a more sophisticated readout, such as reading internal state, write your own function with  the same arguments that returns a vector.  Output vector length must be the same length as specified by output_size in the function call to MDPAgentMachine\n\nArguments: \n\nenv - A MDP following the AbstractEnv interface\n\nReturns:\n\n1.0 if the environment is terminated, 0.0 otherwise.\n\n\n\n\n\n","category":"method"},{"location":"#AlgebraicRL.default_input_function-Tuple{AbstractEnv, Vector}","page":"AlgebraicRL Documentation","title":"AlgebraicRL.default_input_function","text":"default_input_function(env::AbstractEnv, internal_state::Vector)\n\nSets the internal state of the mdp to be the internal state provided. Default function does nothing.  To customize, create your own function with the same arguments. The internal state must be the same size as the size given by input_size in the function call to MDPAgentMachine\n\nArguments:\n\nenv - A MDP following the AbstractEnv interface\ninternal_state - A vector which contains the information needed to set the internal state. In the default_input_function, does nothing.\n\nReturns:\n\nnothing\n\n\n\n\n\n","category":"method"},{"location":"#AlgebraicRL.MDPAgentMachine-Tuple{AbstractEnv, AbstractPolicy}","page":"AlgebraicRL Documentation","title":"AlgebraicRL.MDPAgentMachine","text":"MDPAgentMachine(    env::AbstractEnv, agent::AbstractPolicy,\n                    ;input_size = 1, input_function = default_input_function,\n                    output_size = 1, output_function = default_output_function,\n                    display = false, resetMDP = true\n                    )\n\nThis function returns a DiscreteMachine from AlgebraicDynamics\n\nThe env and agent must follow the respective interfaces from ReinforcementLearning.jl\n\nBy default, the internal workings of the machine does not use the input provided to eval_dynamics. This can be overwritten by using a custom input function. Make sure the size of the vector in your input function is the same as the input_size you provide.\n\nIts output is true if the MDP is terminated and false otherwise. This can also be overwritten by providing a custom output function. Once again, make sure the size of the output vector is the same as the output_size you provide in this function call.\n\nYou can also reset the environment before running the machine if you wish. This is useful in some cases to make sure the environment is in a good state, but it is bad in other cases. So, you are able to control if it is reset or not.\n\nYou can also choose to display the environment each timestep by setting display = true. It will call Base.display(env), so you must overwrite Base.display for your environment in order to use that functionality.\n\nArguments:\n\nenv - a MDP following the AbstractEnv interface\nagent - a agent/policy following the AbstractPolicy interface\ninput_size - an int which must be equal to the size of the vector used by the input function\ninput_function - a function which is called on the MDP before running an episode. Must have the same arguments as default_input_function\noutput_size - an int which must be equal to the size of the vector returned by the output function\noutput_function - a function which is called after the episode is terminated. Must have the same arguments as default_input_function. Returns a vector.\ndisplay - whether or not to display the MDP every timestep. Calls Base.display(env) if true.\nresetMDP - whether or not to reset the MDP via reset!(env) before running an episode.\n\n\n\n\n\n","category":"method"},{"location":"#AlgebraicDynamics.UWDDynam.eval_dynamics","page":"AlgebraicRL Documentation","title":"AlgebraicDynamics.UWDDynam.eval_dynamics","text":"AlgebraicDynamics.DWDDynam.eval_dynamics(f::AbstractMachine, xs=[0.0])\n\nRuns an episode of the MDP following the policy given in the constructor for the machine. Is equilvalent to calling eval_dynamics with zeros for the internal state variable. \n\nArguments: \n\nf - a machine created by MDPAgentMachine\nxs - external variables used for setting the initial state\n\nReturns:\n\na vector of zeros. IE basically nothing but AlgebraicDynamics requires a vector.\n\n\n\n\n\n","category":"function"},{"location":"#AlgebraicRL.read_output-Tuple{AbstractMachine}","page":"AlgebraicRL Documentation","title":"AlgebraicRL.read_output","text":"read_output(f::AbstractMachine)\n\nSince we are not using internal variables via AlgebraicDynamics, we can overwrite readout to assume zeros for those variables. This is convenient for our purposes. However, due to their API structure, we would have issues if we tried to overwite readout(::AbstractMachine). Instead, we will simply rename the function to read_output for our purposes.\n\nArguments:\n\nf - a machine created by MDPAgentMachine\n\nReturns:\n\nThe result of the output_function specified in the function call to MDPAgentMachine\n\n\n\n\n\n","category":"method"}]
}
