var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#MDPAgent","page":"API","title":"MDPAgent","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"MDPAgent\nsolve(mdpAgent::MDPAgent, display = false)\nBase.collect(mdpagent::MDPAgent) \n","category":"page"},{"location":"api/#AlgebraicRL.MDPAgent","page":"API","title":"AlgebraicRL.MDPAgent","text":"mutable struct MDPAgent\n    mdp::AbstractEnv\n    agent::AbstractPolicy\nend\n\nDefines a discrete machine that consists of an MDP and the agent capable of solving it. Running solve() will iterate through 1 episode of the MDP and return a done signal. The MDP follows the specification from ReinforcementLearning.jl. Each MDP, of type AbstractEnv, has the following functions:\n\naction_space(env::YourEnv)\nstate(env::YourEnv)\nstate_space(env::YourEnv)\nreward(env::YourEnv)\nis_terminated(env::YourEnv)\nreset!(env::YourEnv)\n(env::YourEnv)(action) # note this function applies your action to the environment and steps forward in time.\n\nSee https://juliareinforcementlearning.org/docs/Howtowriteacustomized_environment/ for more details on how to write a custom environment. Alternatively, see https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/tree/master/src/ReinforcementLearningExperiments/deps/experiments/experiments for examples of used environments. \n\nThe agent also comes from ReinforcementLearning.jl. See https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/61256bcf1c493914d5003f22e126c997332c2c39/src/ReinforcementLearningBase/src/interface.jl for a full definition. Mainly, it defines the following functions:\n\n(policy::AbstractPolicy)(env) = returns an action from the policy given the environment\nupdate(policy::AbstractPolicy, experience) = updates the policy based on an online or offline experience\n\n\n\n\n\n","category":"type"},{"location":"api/#AlgebraicRL.solve","page":"API","title":"AlgebraicRL.solve","text":"solve(mdpAgent::MDPAgent, display = false)\n\nRuns 1 episode of the environment under the policy provided by the agent.\n\nArguments:\n\nmdpAgent - The MDPAgent you constructed, which contains an MDP following the AbstractEnv interface and an agent following the AbstractPolicy interface.\ndisplay - Whether or not to display the environment each step. Displaying is done via calling Base.display, so overwrite Base.display(yourEnv) in order to view your environment being solved.\n\nReturns:\n\nThe accumulated reward over the episode as returned by the environment\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.collect-Tuple{MDPAgent}","page":"API","title":"Base.collect","text":"Base.collect(mdpagent::MDPAgent)\n\nOverwrites base.collect for our class since it will be used by Algebraic dynamics. Simply returns the mdpAgent since there is nothing to collect. Note collect is usually used to return an array of elements from a collection or iterator \n\n\n\n\n\n","category":"method"},{"location":"api/#MDPAgentMachine","page":"API","title":"MDPAgentMachine","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"MDPAgentMachine(    env::AbstractEnv, agent::AbstractPolicy,\n                        ;input_size = 1, input_function = default_input_function,\n                        output_size = 1, output_function = default_output_function,\n                        display = false, resetMDP = true\n                        )\ndefault_output_function(env::AbstractEnv)\ndefault_input_function(env::AbstractEnv, internal_state::Vector)\n","category":"page"},{"location":"api/#AlgebraicRL.MDPAgentMachine-Tuple{AbstractEnv, AbstractPolicy}","page":"API","title":"AlgebraicRL.MDPAgentMachine","text":"MDPAgentMachine(    env::AbstractEnv, agent::AbstractPolicy;\n                    input_size = 1, input_function = default_input_function,\n                    output_size = 1, output_function = default_output_function,\n                    display = false, resetMDP = true\n                    )\n\nThis function returns a DiscreteMachine from AlgebraicDynamics\n\nThe env and agent must follow the respective interfaces from ReinforcementLearning.jl\n\nBy default, the internal workings of the machine does not use the input provided to eval_dynamics. This can be overwritten by using a custom input function. Make sure the size of the vector in your input function is the same as the input_size you provide.\n\nIts output is true if the MDP is terminated and false otherwise. This can also be overwritten by providing a custom output function. Once again, make sure the size of the output vector is the same as the output_size you provide in this function call.\n\nYou can also reset the environment before running the machine if you wish. This is useful in some cases to make sure the environment is in a good state, but it is bad in other cases. So, you are able to control if it is reset or not.\n\nYou can also choose to display the environment each timestep by setting display = true. It will call Base.display(env), so you must overwrite Base.display for your environment in order to use that functionality.\n\nArguments:\n\nenv - a MDP following the AbstractEnv interface\nagent - a agent/policy following the AbstractPolicy interface\ninput_size - an int which must be equal to the size of the vector used by the input function\ninput_function - a function which is called on the MDP before running an episode. Must have the same arguments as default_input_function\noutput_size - an int which must be equal to the size of the vector returned by the output function\noutput_function - a function which is called after the episode is terminated. Must have the same arguments as default_input_function. Returns a vector.\ndisplay - whether or not to display the MDP every timestep. Calls Base.display(env) if true.\nresetMDP - whether or not to reset the MDP via reset!(env) before running an episode.\n\n\n\n\n\n","category":"method"},{"location":"api/#AlgebraicRL.default_output_function-Tuple{AbstractEnv}","page":"API","title":"AlgebraicRL.default_output_function","text":"default_output_function(env::AbstractEnv)\n\nReturns the result of the mdp episode. Is it done or not?  To do a more sophisticated readout, such as reading internal state, write your own function with  the same arguments that returns a vector.  Output vector length must be the same length as specified by output_size in the function call to MDPAgentMachine\n\nArguments: \n\nenv - A MDP following the AbstractEnv interface\n\nReturns:\n\n1.0 if the environment is terminated, 0.0 otherwise.\n\n\n\n\n\n","category":"method"},{"location":"api/#AlgebraicRL.default_input_function-Tuple{AbstractEnv, Vector}","page":"API","title":"AlgebraicRL.default_input_function","text":"default_input_function(env::AbstractEnv, internal_state::Vector)\n\nSets the internal state of the mdp to be the internal state provided. Default function does nothing.  To customize, create your own function with the same arguments. The internal state must be the same size as the size given by input_size in the function call to MDPAgentMachine\n\nArguments:\n\nenv - A MDP following the AbstractEnv interface\ninternal_state - A vector which contains the information needed to set the internal state. In the default_input_function, does nothing.\n\nReturns:\n\nnothing\n\n\n\n\n\n","category":"method"},{"location":"api/#MDPAgentMachine-and-AlgebraicDynamics.jl","page":"API","title":"MDPAgentMachine and AlgebraicDynamics.jl","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"The following relates to MDPAgentMachines and how they interact with the AlgebraicDynamics.jl interface. We are not using internal variables, which they call 'u', so we are able to simplify the function calls slightly. ","category":"page"},{"location":"api/","page":"API","title":"API","text":"AlgebraicDynamics.DWDDynam.eval_dynamics(f::AbstractMachine, xs=[0.0])\nread_output(f::AbstractMachine)\n\n","category":"page"},{"location":"api/#AlgebraicDynamics.UWDDynam.eval_dynamics","page":"API","title":"AlgebraicDynamics.UWDDynam.eval_dynamics","text":"AlgebraicDynamics.DWDDynam.eval_dynamics(f::AbstractMachine, xs=[0.0])\n\nRuns an episode of the MDP following the policy given in the constructor for the machine. Is equilvalent to calling eval_dynamics with zeros for the internal state variable. \n\nArguments: \n\nf - a machine created by MDPAgentMachine\nxs - external variables used for setting the initial state\n\nReturns:\n\na vector of zeros. IE basically nothing but AlgebraicDynamics requires a vector.\n\n\n\n\n\n","category":"function"},{"location":"api/#AlgebraicRL.read_output-Tuple{AbstractMachine}","page":"API","title":"AlgebraicRL.read_output","text":"read_output(f::AbstractMachine)\n\nSince we are not using internal variables via AlgebraicDynamics, we can overwrite readout to assume zeros for those variables. This is convenient for our purposes. However, due to their API structure, we would have issues if we tried to overwite readout(::AbstractMachine). Instead, we will simply rename the function to read_output for our purposes.\n\nArguments:\n\nf - a machine created by MDPAgentMachine\n\nReturns:\n\nThe result of the output_function specified in the function call to MDPAgentMachine\n\n\n\n\n\n","category":"method"},{"location":"#AlgebraicRL.jl","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"","category":"section"},{"location":"#Introduction","page":"AlgebraicRL.jl","title":"Introduction","text":"","category":"section"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"AlgebraicRL.jl is a library for solving reinforcement learning problems compositionally. Programmatically it relies heavily on ReinforcementLearning.jl, and some understanding of that package is necesarry for using this one. Additionally, this package relies on Catlab.jl and Category Theory, which underpins why compositional reinforcement learning works. Georgios add some explanation here.","category":"page"},{"location":"#How-to-solve-hard-RL-problems-via-AlgebraicRL.jl","page":"AlgebraicRL.jl","title":"How to solve hard RL problems via AlgebraicRL.jl","text":"","category":"section"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"The basic idea of this package is the following. Assume you have some complicated MDP you cannot solve with traditional RL algorithms. To solve it, do the following steps:","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"Break the MDP down into smaller sub-MDPs which are easier to solve\nSolve the sub-MDPs individually\nUsing Wiring Diagrams via Catlab.jl, define the order in which the sub-MDPs should be solved in order to solve the overall problem.\nDefine how the information, typically internal state, will flow between sub-MDPs. This is done via custom functions, see the documentation on MDPAgentMachine.\nUsing the function MDPAgentMachine, create a machine which contains 1 sub-MDP and the policy you have learned to solve it. Create 1 machine per sub-MDP. Use the custom functions from above.\nPlace the machines from above into the wiring diagram via oapply. Use eval_dynamics and read_output to run the composed machine. This composed machine is running each of the sub-MDPs in the order you gave, and therefore is equilvalent to the original, difficult MDP. Assuming you solved the sub-MDPs, and they represent the harder problem, you have now solved that problem. ","category":"page"},{"location":"#Algorithm-In-depth","page":"AlgebraicRL.jl","title":"Algorithm In-depth","text":"","category":"section"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"Note I am using Capital letters to denote a set and lowercase letters to denote an element.","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"Assume we have a Markov Decision Process (MDP) we are trying to solve. The MDP is defined by the following: ","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"an set of initial states S_0. We allow the state space to be either discrete or continuous.\nan action space A. We allow the action space to be either discrete or continuous.\na transition function T(s_t, a_t) which returns a distribution of next states S_t+1 given the state and action taken.\na reward function R(s_t, a_t, s_t+1) which returns a scalar reward.\na set of termination states S_f. Reaching these states causes the MDP to terminate. Note this termination condition could be accomplishing some goal. Alternatively, this could be an empty set for MDPs that do not terminate. ","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"The goal is to learn a policy π, which is a function that maps from s_t to a_t, that maximizes the total accumulated reward of 1 episode of the MDP given that we apply the action returned by π to the MDP at every timestep.","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"Suppose that current algorithms for finding the policy π are not succesful. That is, they are not able to find a function mapping from state to action that leads to a high total accumulated reward. Example algorithms, depending on whether the state and action spaces are discrete or continuous, can consists of tabular approaches, DQN, DDPG, PPO, etc. ","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"Given that this is the case, we must first break the MDP into N sub-MDPs. A sub-MDP is defined to be a MDP, which means its has the same defintion as above. Sub-MDPs however draw many of their elements from the parent MDP:","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"an initial set of states S_0. This set must be a subset of the state space of the parent MDP.\nan action space A. This must be a subset of the action space of the parent MDP.\na transition function T, which is the same as the transtion function of the parent MDP.\na reward function R, which can be independent of the parent MDP\na set of termination states S_f, which must be a subset of the state space of the parent MDP.","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"The N sub-MDPs must have the following properties (note I am assuming 1-indexed):","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"sub-MDP_1's S_0 must be equal to the S_0 of the parent MDP\nsub-MDP_N's S_f must be equal to the S_f of the parent MDP\nFor all intermediate sub-MDPs, the S_f of 1 sub-MDP must be equal to the S_0 of the following sub-MDP.","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"In effect, the first sub-MDP is the beginning of the parent MDP. Reaching the termination states of the first sub-MDP is equilvalent to reaching the initial states of the second sub-MDP. In general, the termination states of the i'th sub-MDP are the initial states of the i+1'th sub-MDP. The termination states of the final sub-MDP are the termination states of the parent MDP. ","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"Therefore, if the human expert has properly dividing the MDP into reasonable sub-MDPs, completing the sub-MDPs sequentially completes the parent MDP. IE completing the first sub-MDP leads to the second. Completing the second leads to the third, and so on, until completing the last sub-MDP leads to the termination states of the parent MDP. ","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"Since each sub-MDP also meets the definition of an MDP, we can apply any algorithm we choose to find a respective π, such as Tabular approaches, DQN, DDPG, PPO, etc. Therefore, we can learn a sub-policy π_i to solve each sub-MDP, and then apply them seqentially to the parent MDP. Thus, the parent MDP can be solved compositionally.","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"Sub-MDPs have many properties that make them easier to solve than the parent MDP:","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"The complexity of the desired behavior can be reduced. Even if the state space and action space are constant, the optimal policy may be much simpler to learn if the behavior of that policy is more general. \nThe state space may be reduced. In some cases, you may need the entire state of the parent MDP to solve the sub-MDP. However, in other cases, only part of the state space is relevant. Any information that is irrelevant can be removed, which makes the problem easier to solve. This is known as information hiding.\nLikewise, the action space may be reduced. In some problems, not all actions are needed to solve a sub-MDP. Removing these actions makes the problem easier to solve as whichever algorithm you choose does not need to explore them. In discrete action spaces, this means removing some actions from the list of actions. In continuous cases, this means setting a predetirmined value for one of the action dimensions. \nThe reward function can be customized for each sub-MDP. This means you can do reward tuning on each sub-MDP individually. This is useful because it is often substantially easier to tune the reward of an easy problem over a complicated one. It is even possible that the parent MDP is so complex it is intractable to create a reward function to encourage the correct behavior. However, it may still be possible to reward tune for sub-MDPs, allowing you to solve a problem that you could not otherwise.","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"Given the above information, the process can be described as the following:","category":"page"},{"location":"","page":"AlgebraicRL.jl","title":"AlgebraicRL.jl","text":"function CompositionalRL(MDP):\n    Break the MDP into N sub-MDPs\n    Find a sub-policy for each sub-MDP\n    Combine the sub-policies into 1 policy π by doing the following:\n        Determine which sub-MDP is currently active \n        Apply the respective sub-policy","category":"page"}]
}
